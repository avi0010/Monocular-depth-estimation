{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-10T19:03:36.446880Z","iopub.execute_input":"2023-04-10T19:03:36.447173Z","iopub.status.idle":"2023-04-10T19:04:06.946574Z","shell.execute_reply.started":"2023-04-10T19:03:36.447145Z","shell.execute_reply":"2023-04-10T19:04:06.945535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.resnet import resnet18 as _resnet34\nimport torch\nimport torch.nn as nn\nimport torchvision as T","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:06.948938Z","iopub.execute_input":"2023-04-10T19:04:06.950887Z","iopub.status.idle":"2023-04-10T19:04:09.274541Z","shell.execute_reply.started":"2023-04-10T19:04:06.950845Z","shell.execute_reply":"2023-04-10T19:04:09.273560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _make_resnet_backbone(resnet):\n    pretrained = nn.Module()\n    pretrained.conv0_0 = nn.Sequential(\n    resnet.conv1, resnet.bn1, resnet.relu, resnet.layer1)\n    pretrained.conv1_0 = resnet.layer2\n    pretrained.conv2_0 = resnet.layer3\n    pretrained.conv3_0 = resnet.layer4\n    return pretrained\n\ndef resnet34(pretrained=True, **kwargs):\n    model = _resnet34(pretrained=pretrained, **kwargs)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:09.276214Z","iopub.execute_input":"2023-04-10T19:04:09.276795Z","iopub.status.idle":"2023-04-10T19:04:09.283408Z","shell.execute_reply.started":"2023-04-10T19:04:09.276756Z","shell.execute_reply":"2023-04-10T19:04:09.281961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Upscale_Block(nn.Module):\n    def __init__(self, input_channel, out):\n        super().__init__()\n        self.cv1 = nn.Conv2d(input_channel, out, 3, stride=(1, 1), padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out)\n        self.cv2 = nn.Conv2d(out, out, 3, stride=(1, 1), padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out)\n        self.relu = nn.ReLU(inplace=True)\n        self.cv3 = nn.Conv2d(out, out, 3, stride=(1, 1), padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out)\n        self.cv4 = nn.Conv2d(out, out, 3, stride=(1, 1), padding=1, bias=False)\n        self.bn4 = nn.BatchNorm2d(out)\n\n    def forward(self, x):\n        out = self.cv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.cv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.cv3(out)\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.cv4(out)\n        out = self.bn4(out)\n        out = self.relu(out)\n\n        return out\n\nclass Interpolate(nn.Module):\n    def __init__(self, scale_factor, mode, align_corners=False):\n        super(Interpolate, self).__init__()\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        self.align_corners = align_corners\n\n    def forward(self, x):\n        x = self.interp(\n            x,\n            scale_factor=self.scale_factor,\n            mode=self.mode,\n            align_corners=self.align_corners,\n        )\n        return x\n    \nclass VGGBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(middle_channels)\n        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:09.286304Z","iopub.execute_input":"2023-04-10T19:04:09.286974Z","iopub.status.idle":"2023-04-10T19:04:09.303516Z","shell.execute_reply.started":"2023-04-10T19:04:09.286937Z","shell.execute_reply":"2023-04-10T19:04:09.302532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DV3(nn.Module):\n    def __init__(self, input_channels=3, **kwargs):\n        super().__init__()\n        nb_filter = [64, 128, 256, 512, 1024]\n        model = resnet34()\n        self.encoder = _make_resnet_backbone(model)\n        #self.conv0_0 = nn.Conv2d(3, 64, 3, padding=1)\n        self.conv4_0 = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(512, 1024, 3, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.MaxPool2d(2, 2),\n        )\n        self.final = nn.Conv2d(nb_filter[0], 1, kernel_size=1)\n        self.up = Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True)\n        self.conv0_1 = VGGBlock(nb_filter[0] + nb_filter[1], nb_filter[0], nb_filter[0])\n        self.conv1_1 = VGGBlock(nb_filter[1] + nb_filter[2], nb_filter[1], nb_filter[1])\n        self.conv2_1 = VGGBlock(nb_filter[2] + nb_filter[3], nb_filter[2], nb_filter[2])\n        self.conv3_1 = Upscale_Block(nb_filter[3] + nb_filter[4], nb_filter[3])\n        self.conv0_2 = VGGBlock(nb_filter[0] * 2 + nb_filter[1], nb_filter[0], nb_filter[0])\n        self.conv1_2 = VGGBlock(nb_filter[1] * 2 + nb_filter[2], nb_filter[1], nb_filter[1])\n        self.conv2_2 = Upscale_Block(nb_filter[2] * 2 + nb_filter[3], nb_filter[2])\n        self.conv0_3 = VGGBlock(nb_filter[0] * 3 + nb_filter[1], nb_filter[0], nb_filter[0])\n        self.conv1_3 = Upscale_Block(nb_filter[1] * 3 + nb_filter[2], nb_filter[1])\n        self.conv0_4 = Upscale_Block(nb_filter[0] * 4 + nb_filter[1], nb_filter[0])\n\n    def forward(self, input):\n        # input_layer = self.conv0_0(input)\n        x0_0 = self.encoder.conv0_0(input)\n        x1_0 = self.encoder.conv1_0(x0_0)\n        x2_0 = self.encoder.conv2_0(x1_0)\n        x3_0 = self.encoder.conv3_0(x2_0)\n        x4_0 = self.conv4_0(x3_0)\n        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n        out = self.up(x0_4)\n        out = self.final(out)\n        return out\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_channels, output_channels, nhead=8, num_layers=6):\n        super(TransformerModel, self).__init__()\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.nhead = nhead\n        self.num_layers = num_layers\n\n        model = _resnet34()\n        self.encoder = _make_resnet_backbone(model)\n        \n        self.transformer_encoder_layers = nn.TransformerEncoderLayer(d_model=512, nhead=nhead)\n        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layers, num_layers=num_layers)\n        self.up = Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True)\n        \n        # self.conv6 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n        self.conv6 = VGGBlock(512, 256, 128)\n        # self.conv7 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        self.conv7 = VGGBlock(128, 64, 32)\n        # self.conv8 = nn.Conv2d(128, output_channels, kernel_size=3, stride=1, padding=1)\n        self.conv8 = VGGBlock(32, 16, output_channels)\n\n    def forward(self, x):\n        x = self.encoder.conv0_0(x)\n        x = self.encoder.conv1_0(x)\n        x = self.encoder.conv2_0(x)\n        x = self.encoder.conv3_0(x)\n        \n        x = x.permute(0, 2, 3, 1)\n        b, h, w, c = x.shape\n        x = x.reshape(b, h*w, c)\n        \n        x = self.transformer_encoder(x)\n        \n        x = x.reshape(b, h, w, c)\n        x = x.permute(0, 3, 1, 2)\n        \n        x = F.relu(self.conv6(self.up(x)))\n        x = F.relu(self.conv7(self.up(x)))\n        x = self.conv8(self.up(x))\n        \n        return self.up(x)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:09.305037Z","iopub.execute_input":"2023-04-10T19:04:09.305399Z","iopub.status.idle":"2023-04-10T19:04:09.330748Z","shell.execute_reply.started":"2023-04-10T19:04:09.305364Z","shell.execute_reply":"2023-04-10T19:04:09.329822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = TransformerModel(input_channels=3, output_channels=1)\n#weights = torch.load(\"/kaggle/input/model4-weight1/model_4(5)_0.0019443425348819468.pth\")\n#lst = [\"encoder\"]\nnetwork = weights.cuda()\nfor name,param in network.named_parameters():\n    param.requires_grad = True\n#for name,param in weights.named_parameters():\n    #print(name)\n    #for i in lst:\n        #if i in name:\n            #param.requires_grad = False\n    #print(name, '---->' ,param.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:09.333835Z","iopub.execute_input":"2023-04-10T19:04:09.334708Z","iopub.status.idle":"2023-04-10T19:04:12.220365Z","shell.execute_reply.started":"2023-04-10T19:04:09.334670Z","shell.execute_reply":"2023-04-10T19:04:12.219358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport os, os.path\nimport cv2\nimport imageio\nimport torch.nn as nn\nimport glob\nimport time\nimport numpy as np\nimport scipy.ndimage\nimport math\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\n%matplotlib inline\n\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.io import read_image","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.222053Z","iopub.execute_input":"2023-04-10T19:04:12.222397Z","iopub.status.idle":"2023-04-10T19:04:12.459681Z","shell.execute_reply.started":"2023-04-10T19:04:12.222363Z","shell.execute_reply":"2023-04-10T19:04:12.458748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp = pd.read_csv('../input/nyu-depth-v2/nyu_data/data/nyu2_train.csv', names=['image', 'label'])","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.461409Z","iopub.execute_input":"2023-04-10T19:04:12.462211Z","iopub.status.idle":"2023-04-10T19:04:12.601057Z","shell.execute_reply.started":"2023-04-10T19:04:12.462171Z","shell.execute_reply":"2023-04-10T19:04:12.599945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp = df_temp.sample(frac = 1 , random_state = 79)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.602588Z","iopub.execute_input":"2023-04-10T19:04:12.602953Z","iopub.status.idle":"2023-04-10T19:04:12.616531Z","shell.execute_reply.started":"2023-04-10T19:04:12.602912Z","shell.execute_reply":"2023-04-10T19:04:12.615585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_temp[:42000]\ndf_val = df_temp[42000:]","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.621103Z","iopub.execute_input":"2023-04-10T19:04:12.621563Z","iopub.status.idle":"2023-04-10T19:04:12.627340Z","shell.execute_reply.started":"2023-04-10T19:04:12.621532Z","shell.execute_reply":"2023-04-10T19:04:12.626230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.reset_index()\ndf_train.drop(['index'] , axis = 1 ,inplace = True)\n\ndf_val = df_val.reset_index()\ndf_val.drop(['index'] , axis = 1 ,inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.629056Z","iopub.execute_input":"2023-04-10T19:04:12.629415Z","iopub.status.idle":"2023-04-10T19:04:12.643935Z","shell.execute_reply.started":"2023-04-10T19:04:12.629379Z","shell.execute_reply":"2023-04-10T19:04:12.643067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomImageDataset(Dataset):\n    def __init__(self, root_dir, data_dir, transform=None, target_transform=None):\n        '''\n        The __init__ function is run once when instantiating the Dataset object\n        '''\n#         self.img_labels = pd.read_csv(annotations_file)\n#         self.img_dir = img_dir\n        self.root_dir = root_dir\n        self.data_dir = data_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        '''\n        The __len__ function returns the number of samples in dataset\n        '''\n        return len(self.data_dir)\n\n    def __getitem__(self, idx):\n        '''\n        The __getitem__ function loads and returns a sample from the dataset at the given index idx\n        '''\n        img_path = os.path.join(self.root_dir, self.data_dir['image'][idx])\n        image = Image.open(img_path)\n        \n\n        label_path = os.path.join(self.root_dir, self.data_dir['label'][idx])\n        label = Image.open(label_path)\n        \n        seed = np.random.randint(2147483647) # make a seed with numpy generator \n        \n        if self.transform is not None:\n            random.seed(seed) # apply this seed to img tranfsorms\n            torch.manual_seed(seed) # needed for torchvision 0.7\n            image = self.transform(image)\n        if self.target_transform is not None:\n            random.seed(seed) # apply this seed to target tranfsorms\n            torch.manual_seed(seed) # needed for  torchvision 0.7\n            label = self.target_transform(label)\n          \n        return image, label","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.645360Z","iopub.execute_input":"2023-04-10T19:04:12.645735Z","iopub.status.idle":"2023-04-10T19:04:12.655046Z","shell.execute_reply.started":"2023-04-10T19:04:12.645700Z","shell.execute_reply":"2023-04-10T19:04:12.654000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = '../input/nyu-depth-v2/nyu_data'\ntrain_dir = '/kaggle/working/train.csv'\nval_dir = '/kaggle/working/val.csv'\ntransform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomApply(transforms=[transforms.ColorJitter(brightness=.3, hue=.1)], p=0.25),\n            transforms.RandomApply(transforms=[transforms.GaussianBlur(kernel_size=(11, 11), sigma=(0.1, 5))], p=0.5),\n            transforms.Resize((512, 512)), # resize, the smaller edge will be matched.\n            transforms.ToTensor(), # convert a PIL image or ndarray to tensor. \n])\n\nval_transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.Resize((512, 512)), # resize, the smaller edge will be matched.\n            transforms.ToTensor(), # convert a PIL image or ndarray to tensor. \n])\n\ntarget_transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.Resize((512, 512)), # resize, the smaller edge will be ma\n            transforms.ToTensor(), # convert a PIL image or ndarray to tensor. \n])","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.656771Z","iopub.execute_input":"2023-04-10T19:04:12.657510Z","iopub.status.idle":"2023-04-10T19:04:12.667251Z","shell.execute_reply.started":"2023-04-10T19:04:12.657467Z","shell.execute_reply":"2023-04-10T19:04:12.666076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomImageDataset(root_dir, df_train, transform=transform, target_transform=target_transform)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.669060Z","iopub.execute_input":"2023-04-10T19:04:12.669447Z","iopub.status.idle":"2023-04-10T19:04:12.677122Z","shell.execute_reply.started":"2023-04-10T19:04:12.669412Z","shell.execute_reply":"2023-04-10T19:04:12.676169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_dataset = CustomImageDataset(root_dir, df_val, transform=val_transform, target_transform=target_transform)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.678828Z","iopub.execute_input":"2023-04-10T19:04:12.679646Z","iopub.status.idle":"2023-04-10T19:04:12.686208Z","shell.execute_reply.started":"2023-04-10T19:04:12.679583Z","shell.execute_reply":"2023-04-10T19:04:12.685426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=11, shuffle=True)\nvalidation_loader = DataLoader(validation_dataset, batch_size=11, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.687692Z","iopub.execute_input":"2023-04-10T19:04:12.688474Z","iopub.status.idle":"2023-04-10T19:04:12.694423Z","shell.execute_reply.started":"2023-04-10T19:04:12.688437Z","shell.execute_reply":"2023-04-10T19:04:12.693669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def vis(img, label):\n #   fig, axs = plt.subplots(1, 4, figsize=(15, 15))\n  #  axs[0].imshow(img[0])\n   # axs[1].imshow(img[1])\n    #axs[2].imshow(transforms.ToPILImage()(img * 255), interpolation=\"bicubic\")\n    #axs[3].imshow(label)\n\n#for v, (i, j) in enumerate(train_loader):\n    #print('batch: ', v)\n    #print(i.size(), torch.max(i), torch.min(i))\n    #print(j.size(), torch.max(j), torch.min(j))\n    #vis(i[0].squeeze(), j[0][0])\n    #plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.696124Z","iopub.execute_input":"2023-04-10T19:04:12.696961Z","iopub.status.idle":"2023-04-10T19:04:12.702734Z","shell.execute_reply.started":"2023-04-10T19:04:12.696924Z","shell.execute_reply":"2023-04-10T19:04:12.701985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\noptimizer = optim.Adam(network.parameters(), lr = 0.0001)\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.704183Z","iopub.execute_input":"2023-04-10T19:04:12.704954Z","iopub.status.idle":"2023-04-10T19:04:12.712380Z","shell.execute_reply.started":"2023-04-10T19:04:12.704918Z","shell.execute_reply":"2023-04-10T19:04:12.711369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_valid_loss = np.inf\nfor epoch in range(20):\n    \n    total_loss = 0\n    #total_correct = 0\n    pbar = tqdm(train_loader)\n    for batch in pbar:\n        if torch.cuda.is_available():\n            images,labels = batch\n            images, labels = images.cuda(), labels.cuda()\n        preds = network(images.float().cuda())\n        t1 = nn.HuberLoss().cuda()\n        #t1 = nn.BCEWithLogitsLoss().cuda()\n        loss = t1(preds, labels.float().cuda())\n        pbar.set_description(f'Loss -> {loss}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.to('cpu').item()\n        #total_correct += get_num_correct(preds, labels)\n        \n    \n    valid_loss = 0.0\n    network.eval()     # Optional when not using Model Specific layer\n    vbar = tqdm(validation_loader)\n    with torch.no_grad():\n        for vbatch in vbar:\n            if torch.cuda.is_available():\n                data, labels = vbatch\n                data, labels = data.cuda(), labels.cuda()\n\n            target = network(data.float()).cuda()\n            t1 = nn.HuberLoss().cuda()\n            loss = t1(target,labels).cuda()\n            valid_loss += loss.to('cpu').item() \n            pbar.set_description(f'Validation Loss -> {loss}')\n            \n    print(f'Epoch {epoch+1} \\t\\t Training Loss: {total_loss} \\t\\t Validation Loss: {valid_loss/len(validation_loader)}')\n    torch.save(network, f\"model_5({epoch+1})_{valid_loss/len(validation_loader)}.pth\")\n        \n    print(f'Epoch ==>> {epoch+1}  \\t\\t Loss ==>> {total_loss/len(train_loader)}')","metadata":{"execution":{"iopub.status.busy":"2023-04-10T19:04:12.714320Z","iopub.execute_input":"2023-04-10T19:04:12.714741Z","iopub.status.idle":"2023-04-10T19:05:34.673198Z","shell.execute_reply.started":"2023-04-10T19:04:12.714706Z","shell.execute_reply":"2023-04-10T19:05:34.671574Z"},"trusted":true},"execution_count":null,"outputs":[]}]}